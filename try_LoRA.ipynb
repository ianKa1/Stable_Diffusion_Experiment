{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 27.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.36.0\",\n",
       "  \"_name_or_path\": \"sd-legacy/stable-diffusion-v1-5\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 1. Config\n",
    "# ============================================\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "dataset_dir = \"./vsr_sd\"        # 你的数据目录，里面有 images/ 和 captions.txt\n",
    "lora_rank = 4\n",
    "train_steps = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "resolution = 512\n",
    "experiment_name = \"sd_lora_bs1_lr1e_4_conv_up\"\n",
    "output_dir = f\"./{experiment_name}\" #MODIFY\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. Load model (Stable Diffusion v1.5)\n",
    "# ============================================\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32 if device == \"mps\" else torch.float16,\n",
    ")\n",
    "pipe.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf44bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_blocks.0.resnets.0.conv1\n",
      "down_blocks.0.resnets.0.conv2\n",
      "down_blocks.0.resnets.1.conv1\n",
      "down_blocks.0.resnets.1.conv2\n",
      "down_blocks.1.resnets.0.conv1\n",
      "down_blocks.1.resnets.0.conv2\n",
      "down_blocks.1.resnets.1.conv1\n",
      "down_blocks.1.resnets.1.conv2\n",
      "down_blocks.2.resnets.0.conv1\n",
      "down_blocks.2.resnets.0.conv2\n",
      "down_blocks.2.resnets.1.conv1\n",
      "down_blocks.2.resnets.1.conv2\n",
      "down_blocks.3.resnets.0.conv1\n",
      "down_blocks.3.resnets.0.conv2\n",
      "down_blocks.3.resnets.1.conv1\n",
      "down_blocks.3.resnets.1.conv2\n",
      "['down_blocks.0.resnets.0.conv1', 'down_blocks.0.resnets.0.conv2', 'down_blocks.0.resnets.1.conv1', 'down_blocks.0.resnets.1.conv2', 'down_blocks.1.resnets.0.conv1', 'down_blocks.1.resnets.0.conv2', 'down_blocks.1.resnets.1.conv1', 'down_blocks.1.resnets.1.conv2', 'down_blocks.2.resnets.0.conv1', 'down_blocks.2.resnets.0.conv2', 'down_blocks.2.resnets.1.conv1', 'down_blocks.2.resnets.1.conv2', 'down_blocks.3.resnets.0.conv1', 'down_blocks.3.resnets.0.conv2', 'down_blocks.3.resnets.1.conv1', 'down_blocks.3.resnets.1.conv2']\n",
      "trainable params: 528,640 || all params: 860,049,604 || trainable%: 0.0615\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): UNet2DConditionModel(\n",
      "      (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (time_proj): Timesteps()\n",
      "      (time_embedding): TimestepEmbedding(\n",
      "        (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "        (act): SiLU()\n",
      "        (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (down_blocks): ModuleList(\n",
      "        (0): CrossAttnDownBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-1): 2 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0-1): 2 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (downsamplers): ModuleList(\n",
      "            (0): Downsample2D(\n",
      "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CrossAttnDownBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-1): 2 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (downsamplers): ModuleList(\n",
      "            (0): Downsample2D(\n",
      "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CrossAttnDownBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-1): 2 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (downsamplers): ModuleList(\n",
      "            (0): Downsample2D(\n",
      "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): DownBlock2D(\n",
      "          (resnets): ModuleList(\n",
      "            (0-1): 2 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (up_blocks): ModuleList(\n",
      "        (0): UpBlock2D(\n",
      "          (resnets): ModuleList(\n",
      "            (0-2): 3 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
      "              (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (upsamplers): ModuleList(\n",
      "            (0): Upsample2D(\n",
      "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CrossAttnUpBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-2): 3 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0-1): 2 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
      "              (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
      "              (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (upsamplers): ModuleList(\n",
      "            (0): Upsample2D(\n",
      "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CrossAttnUpBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-2): 3 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
      "              (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
      "              (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (upsamplers): ModuleList(\n",
      "            (0): Upsample2D(\n",
      "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): CrossAttnUpBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-2): 3 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
      "              (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1-2): 2 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mid_block): UNetMidBlock2DCrossAttn(\n",
      "        (attentions): ModuleList(\n",
      "          (0): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (resnets): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "      (conv_act): SiLU()\n",
      "      (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_module_list = []\n",
    "\n",
    "# MODIFY HERE\n",
    "for name, module in pipe.unet.named_modules():\n",
    "    if 'resnet' in name and 'up_blocks' in name and 'conv' in name and not 'conv_shortcut' in name:\n",
    "        print(name)\n",
    "        lora_module_list.append(name)\n",
    "\n",
    "print(lora_module_list)\n",
    "\n",
    "# Freeze base model\n",
    "pipe.unet.requires_grad_(False)\n",
    "\n",
    "# ============================================\n",
    "# 3. Apply LoRA to UNet cross-attention layers\n",
    "# ============================================\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_rank * 2,\n",
    "    target_modules=lora_module_list, #MODIFY\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "pipe.unet = get_peft_model(pipe.unet, lora_config)\n",
    "pipe.unet.print_trainable_parameters()\n",
    "\n",
    "print(pipe.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe67118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. Dataset\n",
    "# ============================================\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.image_dir = os.path.join(root, \"images\")\n",
    "        self.caption_file = os.path.join(root, \"captions.txt\")\n",
    "\n",
    "        with open(self.caption_file, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        self.data = []\n",
    "        for line in lines:\n",
    "            filename, caption = line.split(\"\\t\")\n",
    "            self.data.append((filename, caption))\n",
    "\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((resolution, resolution)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, caption = self.data[idx]\n",
    "        image = Image.open(os.path.join(self.image_dir, filename)).convert(\"RGB\")\n",
    "        image = self.preprocess(image)\n",
    "        return image, caption\n",
    "\n",
    "\n",
    "dataset = CustomDataset(dataset_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimizer + Scheduler\n",
    "# ============================================\n",
    "optimizer = torch.optim.Adam(pipe.unet.parameters(), lr=learning_rate)\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=train_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0032841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "lora_config.save_pretrained(output_dir)\n",
    "\n",
    "train_config = {\n",
    "    \"lora_rank\": 4,\n",
    "    \"train_steps\": train_steps,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"resolution\": resolution\n",
    "}\n",
    "\n",
    "with open(f\"./{output_dir}/train_config.json\", \"w\") as f:\n",
    "    json.dump(train_config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7116da7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mykmao1515\u001b[0m (\u001b[33mkaimao-columbia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kaimao/Desktop/Stable_Diffusion_Experiment/wandb/run-20251213_161030-4i3felbz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaimao-columbia-university/stable-diffusion-training/runs/4i3felbz' target=\"_blank\">sd_lora_bs1_lr1e_4_conv_down_2</a></strong> to <a href='https://wandb.ai/kaimao-columbia-university/stable-diffusion-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaimao-columbia-university/stable-diffusion-training' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/stable-diffusion-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaimao-columbia-university/stable-diffusion-training/runs/4i3felbz' target=\"_blank\">https://wandb.ai/kaimao-columbia-university/stable-diffusion-training/runs/4i3felbz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kaimao-columbia-university/stable-diffusion-training/runs/4i3felbz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x307f58220>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    project=\"stable-diffusion-training\",   # change this\n",
    "    name=f\"{experiment_name}_2\",           # optional run name\n",
    "    config={\n",
    "        \"train_steps\": train_steps,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"resolution\": resolution,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": lr_scheduler.__class__.__name__,\n",
    "        \"model\": \"sd-legacy/stable-diffusion-v1-5\",\n",
    "        \"train_unet\": True,\n",
    "        \"train_text_encoder\": False,\n",
    "    },\n",
    "    reinit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7266f4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:141: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 / 500, Loss = 0.1404\n",
      "Step 100 / 500, Loss = 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:24<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150 / 500, Loss = 0.6212\n",
      "Step 200 / 500, Loss = 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:33<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250 / 500, Loss = 0.1959\n",
      "Step 300 / 500, Loss = 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:48<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 350 / 500, Loss = 0.1892\n",
      "Step 400 / 500, Loss = 0.1832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:00<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450 / 500, Loss = 0.1596\n",
      "Step 500 / 500, Loss = 0.1775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:01<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished! LoRA saved to: ./sd_lora_bs1_lr1e_4_conv_down\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "VAL_PROMPTS = [\n",
    "    \"a sheep next to a bicycle\",\n",
    "    \"a pig on the bottom of a candle\",\n",
    "]\n",
    "\n",
    "EVAL_SEED = 42\n",
    "EVAL_STEPS = 30\n",
    "EVAL_GUIDANCE = 7.5\n",
    "\n",
    "def save_samples(pipe, step):\n",
    "    out_dir = f\"{output_dir}/train_samples\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    generator = torch.Generator(device=pipe.device).manual_seed(EVAL_SEED)\n",
    "\n",
    "    images = pipe(\n",
    "        EVAL_PROMPTS,\n",
    "        num_inference_steps=EVAL_STEPS,\n",
    "        guidance_scale=EVAL_GUIDANCE,\n",
    "        generator=generator,\n",
    "    ).images\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        img.save(f\"{out_dir}/step_{step:06d}_{EVAL_PROMPTS[i]}.png\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. Training Loop\n",
    "# ============================================\n",
    "pipe.text_encoder.requires_grad_(False)\n",
    "text_encoder = pipe.text_encoder\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "pipe.unet.train()\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(100):  # loop until reaching steps\n",
    "    for batch in dataloader:\n",
    "        if global_step >= train_steps:\n",
    "            break\n",
    "\n",
    "        images, captions = batch\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Encode text\n",
    "        inputs = tokenizer(\n",
    "            list(captions),\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Add noise\n",
    "        with torch.no_grad():\n",
    "            latents = pipe.vae.encode(images).latent_dist.sample()\n",
    "            latents = latents * pipe.vae.config.scaling_factor\n",
    "\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "        # 2. Sample noise\n",
    "        noise = torch.randn_like(latents)\n",
    "\n",
    "        # 3. Add noise according to timestep\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Predict noise\n",
    "        noise_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"loss\": loss.item(),\n",
    "                \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                \"step\": global_step,\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "\n",
    "        if global_step % 50 == 0:\n",
    "            print(f\"Step {global_step} / {train_steps}, Loss = {loss.item():.4f}\")\n",
    "        if global_step % 100 == 0:\n",
    "            pipe.unet.eval()\n",
    "            with torch.no_grad():\n",
    "                save_samples(pipe, global_step)\n",
    "            pipe.unet.train()\n",
    "\n",
    "    if global_step >= train_steps:\n",
    "        break\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. Save LoRA weights\n",
    "# ============================================\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "pipe.unet.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Training finished! LoRA saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbef715",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6de2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/peft/peft_model.py:598: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "# from peft import PeftModel, PeftConfig\n",
    "\n",
    "# pipe.unet = PeftModel.from_pretrained(pipe.unet, \"lora_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:27<00:00,  1.48s/it]\n",
      "100%|██████████| 100/100 [02:31<00:00,  1.52s/it]\n",
      "100%|██████████| 100/100 [02:37<00:00,  1.57s/it]\n",
      "100%|██████████| 100/100 [02:43<00:00,  1.64s/it]\n",
      "100%|██████████| 100/100 [02:49<00:00,  1.70s/it]\n",
      "100%|██████████| 100/100 [02:58<00:00,  1.79s/it]\n",
      "100%|██████████| 100/100 [03:04<00:00,  1.85s/it]\n",
      "100%|██████████| 100/100 [03:10<00:00,  1.91s/it]\n",
      "100%|██████████| 100/100 [03:03<00:00,  1.84s/it]\n",
      "100%|██████████| 100/100 [03:01<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "spatial = [\n",
    "    \"a chicken on the left of a car\",\n",
    "    \"a person on the left of a cow\",\n",
    "    \"a horse on the right of a man\",\n",
    "    \"a man on side of a cat\",\n",
    "    \"a chicken near a book\",\n",
    "    \"a bicycle on the right of a girl\",\n",
    "    \"a dog next to a phone\",\n",
    "    \"a sheep next to a bicycle\",\n",
    "    \"a pig on the bottom of a candle\",\n",
    "    \"a butterfly on the left of a phone\"\n",
    "]\n",
    "\n",
    "inference_dir = f\"{output_dir}/inference\"\n",
    "os.makedirs(inference_dir , exist_ok=True)\n",
    "for prompt in spatial:\n",
    "    image = pipe(prompt).images[0]  \n",
    "    image.save(f\"./{inference_dir}/{prompt}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
