{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f7586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 22.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 797,184 || all params: 860,318,148 || trainable%: 0.0927\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 1. Config\n",
    "# ============================================\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "dataset_dir = \"./vsr_sd\"        # 你的数据目录，里面有 images/ 和 captions.txt\n",
    "lora_rank = 4\n",
    "train_steps = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "resolution = 512\n",
    "output_dir = \"./lora_output\"\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. Load model (Stable Diffusion v1.5)\n",
    "# ============================================\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32 if device == \"mps\" else torch.float16,\n",
    ")\n",
    "pipe.to(device)\n",
    "\n",
    "# Freeze base model\n",
    "pipe.unet.requires_grad_(False)\n",
    "\n",
    "# ============================================\n",
    "# 3. Apply LoRA to UNet cross-attention layers\n",
    "# ============================================\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_rank * 2,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "pipe.unet = get_peft_model(pipe.unet, lora_config)\n",
    "pipe.unet.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. Dataset\n",
    "# ============================================\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.image_dir = os.path.join(root, \"images\")\n",
    "        self.caption_file = os.path.join(root, \"captions.txt\")\n",
    "\n",
    "        with open(self.caption_file, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        self.data = []\n",
    "        for line in lines:\n",
    "            filename, caption = line.split(\"\\t\")\n",
    "            self.data.append((filename, caption))\n",
    "\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((resolution, resolution)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, caption = self.data[idx]\n",
    "        image = Image.open(os.path.join(self.image_dir, filename)).convert(\"RGB\")\n",
    "        image = self.preprocess(image)\n",
    "        return image, caption\n",
    "\n",
    "\n",
    "dataset = CustomDataset(dataset_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimizer + Scheduler\n",
    "# ============================================\n",
    "optimizer = torch.optim.Adam(pipe.unet.parameters(), lr=learning_rate)\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=train_steps,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7266f4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:141: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 / 500, Loss = 0.5066\n",
      "Step 100 / 500, Loss = 0.0054\n",
      "Step 150 / 500, Loss = 0.0121\n",
      "Step 200 / 500, Loss = 0.0305\n",
      "Step 250 / 500, Loss = 0.5797\n",
      "Step 300 / 500, Loss = 0.0741\n",
      "Step 350 / 500, Loss = 0.3555\n",
      "Step 400 / 500, Loss = 0.1010\n",
      "Step 450 / 500, Loss = 0.0277\n",
      "Step 500 / 500, Loss = 0.0053\n",
      "Training finished! LoRA saved to: ./lora_output\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 6. Training Loop\n",
    "# ============================================\n",
    "pipe.text_encoder.requires_grad_(False)\n",
    "text_encoder = pipe.text_encoder\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "pipe.unet.train()\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(10):  # loop until reaching steps\n",
    "    for batch in dataloader:\n",
    "        if global_step >= train_steps:\n",
    "            break\n",
    "\n",
    "        images, captions = batch\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Encode text\n",
    "        inputs = tokenizer(\n",
    "            list(captions),\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Add noise\n",
    "        with torch.no_grad():\n",
    "            latents = pipe.vae.encode(images).latent_dist.sample()\n",
    "            latents = latents * pipe.vae.config.scaling_factor\n",
    "\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "        # 2. Sample noise\n",
    "        noise = torch.randn_like(latents)\n",
    "\n",
    "        # 3. Add noise according to timestep\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Predict noise\n",
    "        noise_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % 50 == 0:\n",
    "            print(f\"Step {global_step} / {train_steps}, Loss = {loss.item():.4f}\")\n",
    "\n",
    "    if global_step >= train_steps:\n",
    "        break\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. Save LoRA weights\n",
    "# ============================================\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "pipe.unet.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Training finished! LoRA saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6de2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/peft/peft_model.py:598: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "pipe.unet = PeftModel.from_pretrained(pipe.unet, \"lora_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f6b7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:06<00:00,  1.33s/it]\n",
      "100%|██████████| 50/50 [01:08<00:00,  1.36s/it]\n",
      "100%|██████████| 50/50 [01:28<00:00,  1.77s/it]\n",
      "100%|██████████| 50/50 [02:29<00:00,  3.00s/it]\n",
      "100%|██████████| 50/50 [02:43<00:00,  3.28s/it]\n",
      "100%|██████████| 50/50 [02:53<00:00,  3.48s/it]\n",
      "100%|██████████| 50/50 [02:45<00:00,  3.32s/it]\n",
      "100%|██████████| 50/50 [02:54<00:00,  3.49s/it]\n",
      "100%|██████████| 50/50 [02:56<00:00,  3.53s/it]\n",
      "100%|██████████| 50/50 [02:50<00:00,  3.41s/it]\n"
     ]
    }
   ],
   "source": [
    "spatial = [\n",
    "    \"a chicken on the left of a car\",\n",
    "    \"a person on the left of a cow\",\n",
    "    \"a horse on the right of a man\",\n",
    "    \"a man on side of a cat\",\n",
    "    \"a chicken near a book\",\n",
    "    \"a bicycle on the right of a girl\",\n",
    "    \"a dog next to a phone\",\n",
    "    \"a sheep next to a bicycle\",\n",
    "    \"a pig on the bottom of a candle\",\n",
    "    \"a butterfly on the left of a phone\"\n",
    "]\n",
    "\n",
    "output_dir = \"spatail_lora_2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for prompt in spatial:\n",
    "    image = pipe(prompt).images[0]  \n",
    "    image.save(f\"./{output_dir}/{prompt}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
