{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 32.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,958,400 || all params: 861,479,364 || trainable%: 0.2273\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 1. Config\n",
    "# ============================================\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "dataset_dir = \"./vsr_sd\"        # 你的数据目录，里面有 images/ 和 captions.txt\n",
    "lora_rank = 4\n",
    "train_steps = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "resolution = 512\n",
    "output_dir = \"./lora_output_conv_all\"\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. Load model (Stable Diffusion v1.5)\n",
    "# ============================================\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32 if device == \"mps\" else torch.float16,\n",
    ")\n",
    "pipe.to(device)\n",
    "\n",
    "# Freeze base model\n",
    "pipe.unet.requires_grad_(False)\n",
    "\n",
    "# ============================================\n",
    "# 3. Apply LoRA to UNet cross-attention layers\n",
    "# ============================================\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_rank * 2,\n",
    "    target_modules=[\"conv1\", \"conv2\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "pipe.unet = get_peft_model(pipe.unet, lora_config)\n",
    "pipe.unet.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. Dataset\n",
    "# ============================================\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.image_dir = os.path.join(root, \"images\")\n",
    "        self.caption_file = os.path.join(root, \"captions.txt\")\n",
    "\n",
    "        with open(self.caption_file, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        self.data = []\n",
    "        for line in lines:\n",
    "            filename, caption = line.split(\"\\t\")\n",
    "            self.data.append((filename, caption))\n",
    "\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((resolution, resolution)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, caption = self.data[idx]\n",
    "        image = Image.open(os.path.join(self.image_dir, filename)).convert(\"RGB\")\n",
    "        image = self.preprocess(image)\n",
    "        return image, caption\n",
    "\n",
    "\n",
    "dataset = CustomDataset(dataset_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimizer + Scheduler\n",
    "# ============================================\n",
    "optimizer = torch.optim.Adam(pipe.unet.parameters(), lr=learning_rate)\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=train_steps,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf18134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): UNet2DConditionModel(\n",
      "      (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (time_proj): Timesteps()\n",
      "      (time_embedding): TimestepEmbedding(\n",
      "        (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "        (act): SiLU()\n",
      "        (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "      (down_blocks): ModuleList(\n",
      "        (0): CrossAttnDownBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-1): 2 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0-1): 2 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (downsamplers): ModuleList(\n",
      "            (0): Downsample2D(\n",
      "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CrossAttnDownBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-1): 2 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (downsamplers): ModuleList(\n",
      "            (0): Downsample2D(\n",
      "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CrossAttnDownBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-1): 2 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (downsamplers): ModuleList(\n",
      "            (0): Downsample2D(\n",
      "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): DownBlock2D(\n",
      "          (resnets): ModuleList(\n",
      "            (0-1): 2 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (up_blocks): ModuleList(\n",
      "        (0): UpBlock2D(\n",
      "          (resnets): ModuleList(\n",
      "            (0-2): 3 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(2560, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (upsamplers): ModuleList(\n",
      "            (0): Upsample2D(\n",
      "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CrossAttnUpBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-2): 3 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0-1): 2 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(2560, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1920, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (upsamplers): ModuleList(\n",
      "            (0): Upsample2D(\n",
      "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CrossAttnUpBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-2): 3 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1920, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(960, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (upsamplers): ModuleList(\n",
      "            (0): Upsample2D(\n",
      "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): CrossAttnUpBlock2D(\n",
      "          (attentions): ModuleList(\n",
      "            (0-2): 3 x Transformer2DModel(\n",
      "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (transformer_blocks): ModuleList(\n",
      "                (0): BasicTransformerBlock(\n",
      "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn1): Attention(\n",
      "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn2): Attention(\n",
      "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
      "                    (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
      "                    (to_out): ModuleList(\n",
      "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "                  (ff): FeedForward(\n",
      "                    (net): ModuleList(\n",
      "                      (0): GEGLU(\n",
      "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                      )\n",
      "                      (1): Dropout(p=0.0, inplace=False)\n",
      "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "          (resnets): ModuleList(\n",
      "            (0): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(960, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1-2): 2 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "              (conv1): lora.Conv2d(\n",
      "                (base_layer): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(640, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): lora.Conv2d(\n",
      "                (base_layer): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv2d(4, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mid_block): UNetMidBlock2DCrossAttn(\n",
      "        (attentions): ModuleList(\n",
      "          (0): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                  (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                  (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (resnets): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "            (conv1): lora.Conv2d(\n",
      "              (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): lora.Conv2d(\n",
      "              (base_layer): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Conv2d(1280, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Conv2d(4, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "      (conv_act): SiLU()\n",
      "      (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pipe.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "lora_config.save_pretrained(output_dir)\n",
    "\n",
    "train_config = {\n",
    "    \"lora_rank\": 4,\n",
    "    \"train_steps\": train_steps,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"resolution\": resolution\n",
    "}\n",
    "\n",
    "with open(f\"./{output_dir}/train_config.json\", \"w\") as f:\n",
    "    json.dump(train_config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7266f4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/diffusers/configuration_utils.py:141: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 / 2000, Loss = 0.0838\n",
      "Step 100 / 2000, Loss = 0.0276\n",
      "Step 150 / 2000, Loss = 0.7123\n",
      "Step 200 / 2000, Loss = 0.0116\n",
      "Step 250 / 2000, Loss = 0.1197\n",
      "Step 300 / 2000, Loss = 0.0057\n",
      "Step 350 / 2000, Loss = 0.1925\n",
      "Step 400 / 2000, Loss = 0.0505\n",
      "Step 450 / 2000, Loss = 0.0771\n",
      "Step 500 / 2000, Loss = 0.1492\n",
      "Step 550 / 2000, Loss = 0.4443\n",
      "Step 600 / 2000, Loss = 0.0078\n",
      "Step 650 / 2000, Loss = 0.0941\n",
      "Step 700 / 2000, Loss = 0.0290\n",
      "Step 750 / 2000, Loss = 0.2600\n",
      "Step 800 / 2000, Loss = 0.2163\n",
      "Step 850 / 2000, Loss = 0.1387\n",
      "Step 900 / 2000, Loss = 0.0219\n",
      "Step 950 / 2000, Loss = 0.1483\n",
      "Step 1000 / 2000, Loss = 0.0756\n",
      "Step 1050 / 2000, Loss = 0.3233\n",
      "Step 1100 / 2000, Loss = 0.2288\n",
      "Step 1150 / 2000, Loss = 0.0265\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(noise_pred, noise)\n\u001b[1;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"stable-diffusion-training\",   # change this\n",
    "    name=\"sd-lora-unet-bs1-lr1e4-conv-all\",           # optional run name\n",
    "    config={\n",
    "        \"train_steps\": train_steps,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"resolution\": resolution,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"scheduler\": lr_scheduler.__class__.__name__,\n",
    "        \"model\": \"sd-legacy/stable-diffusion-v1-5\",\n",
    "        \"train_unet\": True,\n",
    "        \"train_text_encoder\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. Training Loop\n",
    "# ============================================\n",
    "pipe.text_encoder.requires_grad_(False)\n",
    "text_encoder = pipe.text_encoder\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "pipe.unet.train()\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(100):  # loop until reaching steps\n",
    "    for batch in dataloader:\n",
    "        if global_step >= train_steps:\n",
    "            break\n",
    "\n",
    "        images, captions = batch\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Encode text\n",
    "        inputs = tokenizer(\n",
    "            list(captions),\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Add noise\n",
    "        with torch.no_grad():\n",
    "            latents = pipe.vae.encode(images).latent_dist.sample()\n",
    "            latents = latents * pipe.vae.config.scaling_factor\n",
    "\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "        # 2. Sample noise\n",
    "        noise = torch.randn_like(latents)\n",
    "\n",
    "        # 3. Add noise according to timestep\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Predict noise\n",
    "        noise_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"loss\": loss.item(),\n",
    "                \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                \"step\": global_step,\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "\n",
    "        if global_step % 50 == 0:\n",
    "            print(f\"Step {global_step} / {train_steps}, Loss = {loss.item():.4f}\")\n",
    "\n",
    "    if global_step >= train_steps:\n",
    "        break\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. Save LoRA weights\n",
    "# ============================================\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "pipe.unet.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Training finished! LoRA saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85b9c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished! LoRA saved to: ./lora_output_conv1\n"
     ]
    }
   ],
   "source": [
    "pipe.unet.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Training finished! LoRA saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6de2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/Users/kaimao/Desktop/Stable_Diffusion_Experiment/.venv/lib/python3.10/site-packages/peft/peft_model.py:598: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_A.default.weight', 'base_model.model.base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "# from peft import PeftModel, PeftConfig\n",
    "\n",
    "# pipe.unet = PeftModel.from_pretrained(pipe.unet, \"lora_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6b7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:27<00:00,  1.48s/it]\n",
      "100%|██████████| 100/100 [02:31<00:00,  1.52s/it]\n",
      "100%|██████████| 100/100 [02:37<00:00,  1.57s/it]\n",
      "100%|██████████| 100/100 [02:43<00:00,  1.64s/it]\n",
      "100%|██████████| 100/100 [02:49<00:00,  1.70s/it]\n",
      "100%|██████████| 100/100 [02:58<00:00,  1.79s/it]\n",
      "100%|██████████| 100/100 [03:04<00:00,  1.85s/it]\n",
      "100%|██████████| 100/100 [03:10<00:00,  1.91s/it]\n",
      "100%|██████████| 100/100 [03:03<00:00,  1.84s/it]\n",
      "100%|██████████| 100/100 [03:01<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "spatial = [\n",
    "    \"a chicken on the left of a car\",\n",
    "    \"a person on the left of a cow\",\n",
    "    \"a horse on the right of a man\",\n",
    "    \"a man on side of a cat\",\n",
    "    \"a chicken near a book\",\n",
    "    \"a bicycle on the right of a girl\",\n",
    "    \"a dog next to a phone\",\n",
    "    \"a sheep next to a bicycle\",\n",
    "    \"a pig on the bottom of a candle\",\n",
    "    \"a butterfly on the left of a phone\"\n",
    "]\n",
    "\n",
    "output_dir = f\"spatail_lora_{output_dir}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for prompt in spatial:\n",
    "    image = pipe(prompt).images[0]  \n",
    "    image.save(f\"./{output_dir}/{prompt}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
